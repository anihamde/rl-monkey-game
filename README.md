In order to run the script and induce the computer to start learning, run stub.py in the Terminal window. Before doing that, though, it is important to check the values of a few key hyperparameters. These hyperparameters can be found in the stub.py file. The call to the ReinforcementLearning class in line 22 is where one should first focus. Within it are two parameters, the learning rate and the discount rate. Altering these can lead to very different learning and performance outcomes. In addition, the epsilon value can be varied to induce different mixes of exploration and exploitation. A larger value of epsilon leads to a higher probability of taking a random action and exploring in the epsilon-greedy algorithm under the SARSA framework. In combination with the epsilon value, one can change the status of the function that determines our epsilon-greedy configuration. This function is found in line 48 and is currently set to an exponential function that approaches 0 rapidly after a certain total score has been accumulated. Placing the basis of when to start leaving behind epsilon-greedy and when to start mirroring an off-policy maximization approach comes from the notion that the total score accumulated can serve as a proxy for how much our method has improved via learning. However, other functions and values can be inputted to experiment with different configurations of epsilon-greedy.